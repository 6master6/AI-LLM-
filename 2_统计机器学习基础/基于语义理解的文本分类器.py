import os
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np


positive_samples = [
    "根据国家统计局发布的数据，2023 年 GDP 增长率达到 5.2%，高于市场预期。",
    "实验组与对照组在 p<0.05 的水平上差异显著，说明新药疗效确切。",
    "本文采用随机对照双盲设计，样本量 400 人，统计功效 90%。",
    "该算法时间复杂度 O(n log n)，空间复杂度 O(1)，满足工业部署需求。",
    "联合国报告指出，全球平均气温较工业化前已上升 1.1℃。",
    "对 10 万条用户日志进行回归分析，发现点击率与页面加载时长呈显著负相关。",
    "依据《民法典》第 1032 条，该合同条款无效，因其排除了对方主要权利。",
    "从供需曲线来看，补贴将造成无谓损失，降低社会总福利。",
    "图 3 显示了三种模型在 F1 分数上的对比，BERT-base 领先 2.3 个百分点。",
    "该芯片采用 7nm 工艺，晶体管数量 120 亿，功耗降低 35%。",
    "根据考古碳十四测定，该遗址距今约 3200 年，误差 ±50 年。",
    "研究样本覆盖 28 个省份，男女比例 1:1，年龄分布 18-65 岁。",
    "采用双重差分法控制混杂变量后，政策效应估计值为 0.18。",
    "表 2 展示了各变量 VIF 值，均小于 5，不存在严重多重共线性。",
    "该结论在 1%、5%、10% 显著性水平下均稳健。",
    "经质谱分析，样品主要成分为 SiO₂，含量 97.3%，杂质低于 0.1%。",
    "根据牛顿第二定律 F=ma，可计算加速度为 9.8 m/s²。",
    "问卷 Cronbach α 系数 0.87，信度良好。",
    "图神经网络在节点分类任务上的 AUC 提升至 0.941。",
    "对比三种排序算法，快速排序平均耗时最短，仅为 12 ms。",
    "该法律条文采用“合理人”标准，而非主观故意。",
    "使用 LSTM 模型对股价进行预测，RMSE 降至 0.023。",
    "观测数据显示，哈勃常数约为 70 km/s/Mpc。",
    "通过扫描电镜，可观察到材料表面致密无裂纹。",
    "实验重复 5 次，结果稳定，标准差 0.03。",
    "该定理证明见附录 A，采用数学归纳法。",
    "对 3 年面板数据使用固定效应模型，消除了不随时间变化的异质性。",
    "从宏观角度看，M2 同比增速与 CPI 存在 2 个季度滞后期。",
    "采用蒙特卡罗模拟 10 万次，估计置信区间 [0.42,0.48]。",
    "该函数满足 Lipschitz 连续，常数 L=0.8。",
    "审计报告认为，公司财务报表在所有重大方面公允列报。",
    "根据 Ohm 定律，可推算电阻为 4.7 kΩ。",
    "该模型在 ImageNet-1k 上的 Top-1 准确率达到 84.7%。",
    "临床试验遵循 CONSORT 指南，注册号 ChiCTR2200065432。",
    "通过差示扫描量热法测得熔点为 156.3 ℃。",
    "该回归模型 R² 为 0.63，解释力度适中。",
    "采用贝叶斯信息准则 BIC 进行模型选择，最优滞后阶数为 2。",
    "观测值残差近似正态，Shapiro-Wilk 检验 p=0.21。",
    "该法律判例确立了“最小必要”原则，被后续广泛引用。",
    "实验设置 3 个梯度浓度，每组 6 个平行样。",
    "根据热力学第一定律，系统内能变化 ΔU = Q - W。",
    "使用 A/B 测试，实验组转化率提升 4.6%，统计显著。",
    "该地图投影保持面积不变，适合展示分布密度。",
    "通过 X 射线衍射，确认晶体结构为面心立方。",
    "利用皮尔逊相关系数，发现两者 r=0.82，呈强正相关。",
    "该文献综述共纳入 127 篇同行评议文章。",
    "在 95% 置信水平下，总体均值介于 3.9 到 4.4 之间。",
    "该算法通过剪枝减少 60% 计算量，不损失精度。",
    "采用 Granger 因果检验，货币供应是通胀的短期原因。",
    "该论文被 SCI 一区期刊接收，影响因子 8.3。",
    "通过控制变量法，我们排除了温度对反应的干扰。",
    "根据 Bayes 定理，后验概率更新为 0.73。",
    "该调查采用分层抽样，保证城乡比例 1:1。",
    "对数秩检验显示两组生存曲线差异显著，p=0.007。"
]

negative_samples = [
    "哇，这部电影真的让我哭成狗，太感动了！",
    "我就是喜欢这种小确幸的感觉，暖暖的。",
    "啊啊啊，哥哥跳舞也太帅了吧，疯狂打 call！",
    "想到明天放假，心里就乐开了花。",
    "看到落日余晖洒在湖面，我突然觉得人生好美好。",
    "老板今天居然夸了我，开心到飞起！",
    "每次听到这首歌，都会想起那年夏天的海风。",
    "好烦啊，地铁又坏了，今天肯定要迟到了。",
    "为什么他总是不懂我，好委屈。",
    "这家咖啡拉花也太可爱了吧，舍不得喝。",
    "深夜刷到前任的动态，瞬间破防了。",
    "今天的晚霞像被打翻的调色盘，浪漫得我窒息。",
    "真的太惊喜了，没想到她会给我准备生日礼物！",
    "我觉得我就是全世界最幸福的人！",
    "这种孤独感像潮水一样把我淹没。",
    "看到小朋友牵着气球跑，心都要化了。",
    "为什么努力没有回报，好沮丧。",
    "他的笑容像阳光一样照进我心里。",
    "一想到要早起上班，整个人都不好了。",
    "这个配色也太好看了吧，少女心爆棚。",
    "听着雨声，思绪飘回小时候外婆家的屋檐。",
    "被喜欢的人点赞了朋友圈，激动到原地转圈。",
    "那一刻，我觉得整个世界都安静了。",
    "我真的好怕黑，夜里一个人都不敢睡。",
    "看到流浪猫蹭我的腿，瞬间被治愈。",
    "今天心情不好，只想窝在床上发呆。",
    "他一句“晚安”就能让我安心入睡。",
    "为什么生活这么难，想哭。",
    "这个蛋糕甜到心里，幸福感爆棚。",
    "风吹过发梢，好像他在轻轻摸我的头。",
    "突然好想家，眼泪止不住地流。",
    "每次路过这条街，都会想起我们一起走的日子。",
    "被喜欢的人拒绝，心像被撕裂一样痛。",
    "听着老歌，回忆像电影一样在脑海播放。",
    "今天的阳光刚刚好，适合想你。",
    "收到远方朋友的明信片，感动到泪目。",
    "我可能就是天生慢热，才一直交不到朋友。",
    "看到彩虹那一刻，我许愿我们能永远在一起。",
    "夜深了，耳机里循环的还是那首情歌。",
    "真的好累，想逃离这一切。",
    "他的背影渐行渐远，我却连一句再见都说不出口。",
    "这个冬天好冷，连拥抱都显得奢侈。",
    "看到孩子的笑脸，一天的疲惫都没了。",
    "为什么我总是那么容易陷入情绪低谷。",
    "那一刻，我仿佛听见了自己的心跳。",
    "生活不止眼前的苟且，还有诗和远方的田野。",
    "被老板当众批评，委屈到想辞职。",
    "这个香味让我想起妈妈做的红烧肉。",
    "每次听到这首旋律，都会忍不住跟着哼唱。",
    "今天的天空像漫画一样梦幻，忍不住拍了 100 张。",
    "原来真正难过的时候，是哭不出声的。",
    "看到情侣牵手，突然觉得自己好孤单。",
    "这个娃娃也太可爱了吧，想立刻抱回家。",
    "有时候，沉默比争吵更让人心碎。",
    "阳光照进教室，尘埃飞舞得像金色的雪。"
]

texts = positive_samples + negative_samples
labels = [1]*len(positive_samples) + [0]*len(negative_samples)


os.environ['http_proxy'] = 'socks5h://127.0.0.1:1080'
os.environ['https_proxy'] = 'socks5h://127.0.0.1:1080'



model_name = "BAAI/bge-m3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).eval()

@torch.inference_mode()
def embed(texts, batch_size=8):
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        tok = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors="pt")
        out = model(**tok)
        # 取 [CLS] 向量 + 归一化
        vec = out.last_hidden_state[:, 0]
        vec = torch.nn.functional.normalize(vec, p=2, dim=1)
        embs.append(vec.cpu().numpy())
    return np.vstack(embs)

X = embed(texts)
y = np.array(labels)
print("Embeddings shape:", X.shape)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

clf = LogisticRegression(max_iter=1000)
clf.fit(X, y)

# 简单留一法评估（样本量不大，用全量训练即可）
pred = clf.predict(X)
print(classification_report(y, pred, target_names=["Subjective", "Objective"]))
print("Confusion matrix:\n", confusion_matrix(y, pred))

test_sentences = [
    "根据最新财报，公司净利润同比增长 37%，现金流稳健。",
    "今天心情糟透了，什么都不想做。",
    "利用傅里叶变换可将信号从时域映射到频域。",
    "风吹过麦浪，像金色的海洋在跳舞，美到窒息。"
]

test_emb = embed(test_sentences)
pred_test = clf.predict(test_emb)
for s, p in zip(test_sentences, pred_test):
    print("句子:", s)
    print("→ 预测风格:", "Objective" if p==1 else "Subjective", "\n")